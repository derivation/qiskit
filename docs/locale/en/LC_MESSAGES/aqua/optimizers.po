# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2019, Qiskit Development Team
# This file is distributed under the same license as the Qiskit package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2019.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Qiskit \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2019-06-12 14:30+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.7.0\n"

#: ../../aqua/optimizers.rst:5
msgid "Optimizers"
msgstr ""

#: ../../aqua/optimizers.rst:7
msgid ""
"Aqua  contains a variety of classical optimizers for use by quantum "
"variational algorithms, such as :ref:`vqe`. Logically, these optimizers "
"can be divided into two categories:"
msgstr ""

#: ../../aqua/optimizers.rst:11
msgid ""
":ref:`Local Optimizers`: Given an optimization problem, a *local "
"optimizer* is a function that attempts to find an optimal value within "
"the neighboring set of a candidate solution."
msgstr ""

#: ../../aqua/optimizers.rst:14
msgid ""
":ref:`Global Optimizers`: Given an optimization problem, a *global "
"optimizer* is a function that attempts to find an optimal value among all"
" possible solutions."
msgstr ""

msgid "Extending the Optimizer Library"
msgstr ""

#: ../../aqua/optimizers.rst:20
msgid ""
"Consistent with its unique  design, Aqua has a modular and extensible "
"architecture. Algorithms and their supporting objects, such as optimizers"
" for quantum variational algorithms, are pluggable modules in Aqua. New "
"optimizers for quantum variational algorithms are typically installed in "
"the ``qiskit_aqua/utils/optimizers`` folder and derive from the "
"``Optimizer`` class.  Aqua also allows for :ref:`aqua-dynamically-"
"discovered-components`: new optimizers can register themselves as Aqua "
"extensions and be dynamically discovered at run time independent of their"
" location in the file system. This is done in order to encourage "
"researchers and developers interested in :ref:`aqua-extending` to extend "
"the Aqua framework with their novel research contributions."
msgstr ""

#: ../../aqua/optimizers.rst:34
msgid ""
"`Section :ref:`aqua-extending` provides more details on how to extend "
"Aqua with new components."
msgstr ""

#: ../../aqua/optimizers.rst:41
msgid "Local Optimizers"
msgstr ""

#: ../../aqua/optimizers.rst:43
msgid ""
"This section presents the classical local optimizers made available in "
"Aqua. These optimizers are meant to be used in conjunction with quantum "
"variational algorithms:"
msgstr ""

#: ../../aqua/optimizers.rst:47
msgid ":ref:`ADAM`"
msgstr ""

#: ../../aqua/optimizers.rst:48
msgid ":ref:`Analytic Quantum Gradient Descent (AQGD)`"
msgstr ""

#: ../../aqua/optimizers.rst:49
msgid ":ref:`Conjugate Gradient (CG) Method`"
msgstr ""

#: ../../aqua/optimizers.rst:50
msgid ":ref:`Constrained Optimization BY Linear Approximation (COBYLA)`"
msgstr ""

#: ../../aqua/optimizers.rst:51
msgid ":ref:`Limited-memory Broyden-Fletcher-Goldfarb-Shanno Bound (L-BFGS-B)`"
msgstr ""

#: ../../aqua/optimizers.rst:52
msgid ":ref:`Nelder-Mead`"
msgstr ""

#: ../../aqua/optimizers.rst:53
msgid ":ref:`Parallel Broyden-Fletcher-Goldfarb-Shann (P-BFGS)`"
msgstr ""

#: ../../aqua/optimizers.rst:54
msgid ":ref:`Powell`"
msgstr ""

#: ../../aqua/optimizers.rst:55
msgid ":ref:`Sequential Least SQuares Programming (SLSQP)`"
msgstr ""

#: ../../aqua/optimizers.rst:56
msgid ":ref:`Simultaneous Perturbation Stochastic Approximation (SPSA)`"
msgstr ""

#: ../../aqua/optimizers.rst:57
msgid ":ref:`Truncated Newton (TNC)`"
msgstr ""

#: ../../aqua/optimizers.rst:59
msgid ""
"Except for :ref:`ADAM`, :ref:`Analytic Quantum Gradient Descent (AQGD)` "
"and :ref:`Parallel Broyden-Fletcher-Goldfarb-Shann (P-BFGS)`, all these "
"optimizers are directly based on the ``scipy.optimize.minimize`` "
"optimization function in the `SciPy "
"<https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html>`__"
" Python library. They all have a common pattern for parameters. "
"Specifically, the ``tol`` parameter, whose value must be a ``float`` "
"indicating *tolerance for termination*, is from the "
"``scipy.optimize.minimize``  method itself, while the remaining "
"parameters are from the `options dictionary "
"<https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.show_options.html>`__,"
" which may be referred to for further information."
msgstr ""

msgid "Transparent Parallelization of Gradient-based Local Opitmizers"
msgstr ""

#: ../../aqua/optimizers.rst:72
msgid ""
"Aqua comes with a large collection of adaptive algorithms, such as the "
"`Variational Quantum Eigensolver (VQE) algorithm "
"<https://www.nature.com/articles/ncomms5213>`__, `Quantum Approximate "
"Optimization Algorithm (QAOA) <https://arxiv.org/abs/1411.4028>`__, the "
"`Quantum Support Vector Machine (SVM) Variational Algorithm "
"<https://arxiv.org/abs/1804.11326>`__ for AI. All these algorithms "
"interleave quantum and classical computations, making use of classical "
"optimizers. Aqua includes nine local and five global optimizers to choose"
" from. By profiling the execution of the adaptive algorithms, we have "
"detected that a large portion of the execution time is taken by the "
"optimization phase, which runs classically. Among the most widely used "
"optimizers are the *gradient-based* ones; these optimizers attempt to "
"compute the absolute minimum (or maximum) of a function :math:`f` through"
" its gradient."
msgstr ""

#: ../../aqua/optimizers.rst:87
msgid ""
"Seven local optimizers among those integrated into Aqua are gradient-"
"based: the four local optimizers *Limited-memory Broyden-Fletcher-"
"Goldfarb-Shanno Bound (L-BFGS-B)*, *Sequential Least SQuares Programming "
"(SLSQP)*, *Conjugate Gradient (CG)*, and *Truncated Newton (TNC)* from "
"`SciPy "
"<https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html>`__,"
" as well as `Simultaneous Perturbation Stochastic Approximation (SPSA) "
"<https://www.jhuapl.edu/SPSA/>`__, *ADAM* and *Analytic Quantum Gradient "
"Descent (AQGD)*. Aqua contains a methodology that parallelizes the "
"classical computation of the partial derivatives in the gradient-based "
"local optimizers listed above. This parallelization takes place "
"*transparently*, in the sense that Aqua intercepts the computation of the"
" partial derivatives and parallelizes it without making any change to the"
" actual source code of the optimizers."
msgstr ""

#: ../../aqua/optimizers.rst:101
#, python-format
msgid ""
"In order to activate the parallelization mechanism for an adaptive "
"algorithm included in Aqua, it is sufficient to construct it with "
"parameter ``batch_mode`` set to ``True``. Our experiments have proven "
"empirically that parallelizing the process of a gradient-based local "
"optimizer achieves a 30% speedup in the execution time of an adaptive "
"algorithms on a simulator."
msgstr ""

#: ../../aqua/optimizers.rst:112
msgid "ADAM"
msgstr ""

#: ../../aqua/optimizers.rst:113
msgid ""
"ADAM is a gradient-based optimization algorithm that is relies on "
"adaptive estimates of lower-order moments. The algorithm requires little "
"memory and is invariant to diagonal rescaling of the gradients. "
"Furthermore, it is able to cope with non-stationary objective functions "
"and noisy and/or sparse gradients. AMSGRAD (a variant of ADAM) uses a "
"'long-term memory' of past gradients and, thereby, improves convergence "
"properties."
msgstr ""

#: ../../aqua/optimizers.rst:119
msgid ""
"Kingma, Diederik & Ba, Jimmy. (2014). Adam: A Method for Stochastic "
"Optimization. International Conference on Learning Representations."
msgstr ""

#: ../../aqua/optimizers.rst:122
msgid ""
"Sashank J. Reddi and Satyen Kale and Sanjiv Kumar. (2018). On the "
"Convergence of Adam and Beyond. International Conference on Learning "
"Representations."
msgstr ""

#: ../../aqua/optimizers.rst:125 ../../aqua/optimizers.rst:220
#: ../../aqua/optimizers.rst:280 ../../aqua/optimizers.rst:396
#: ../../aqua/optimizers.rst:456 ../../aqua/optimizers.rst:567
#: ../../aqua/optimizers.rst:632 ../../aqua/optimizers.rst:797
msgid "The following parameters are supported:"
msgstr ""

#: ../../aqua/optimizers.rst:127 ../../aqua/optimizers.rst:222
msgid "The maximum number of iterations to perform."
msgstr ""

#: ../../aqua/optimizers.rst:133 ../../aqua/optimizers.rst:288
msgid "This parameters takes a positive ``int`` value.  The default is ``20``."
msgstr ""

#: ../../aqua/optimizers.rst:135 ../../aqua/optimizers.rst:237
msgid "The tolerance for termination. .. code:: python"
msgstr ""

#: ../../aqua/optimizers.rst:138 ../../aqua/optimizers.rst:240
msgid "tol : float"
msgstr ""

#: ../../aqua/optimizers.rst:140 ../../aqua/optimizers.rst:242
msgid "The default value is ``1e-06``."
msgstr ""

#: ../../aqua/optimizers.rst:142 ../../aqua/optimizers.rst:230
msgid "The learning rate: .. code:: python"
msgstr ""

#: ../../aqua/optimizers.rst:145
msgid "lr : float"
msgstr ""

#: ../../aqua/optimizers.rst:147
msgid "The default value is ``1e-03``."
msgstr ""

#: ../../aqua/optimizers.rst:149
msgid ""
"First hyper-parameter used for the evaluation of the first moment "
"estimate. .. code:: python"
msgstr ""

#: ../../aqua/optimizers.rst:152
msgid "beta_1 : float"
msgstr ""

#: ../../aqua/optimizers.rst:154
msgid "The default value is ``0.9``."
msgstr ""

#: ../../aqua/optimizers.rst:156
msgid ""
"Second hyper-parameter used for the evaluation of the second moment "
"estimate. .. code:: python"
msgstr ""

#: ../../aqua/optimizers.rst:159
msgid "beta_2 : float"
msgstr ""

#: ../../aqua/optimizers.rst:161
msgid "The default value is ``0.99``."
msgstr ""

#: ../../aqua/optimizers.rst:163
msgid "Noise factor used for reasons of numerical stability."
msgstr ""

#: ../../aqua/optimizers.rst:169
msgid "The default value is ``1e-8``."
msgstr ""

#: ../../aqua/optimizers.rst:171 ../../aqua/optimizers.rst:317
#: ../../aqua/optimizers.rst:668
msgid "Step size used for numerical approximation of the Jacobian."
msgstr ""

#: ../../aqua/optimizers.rst:177
msgid "The default value is ``1e-10``."
msgstr ""

#: ../../aqua/optimizers.rst:179
msgid "A Boolean value indicating whether or not to use the AMSGRAD variant."
msgstr ""

#: ../../aqua/optimizers.rst:185 ../../aqua/optimizers.rst:251
#: ../../aqua/optimizers.rst:296 ../../aqua/optimizers.rst:354
#: ../../aqua/optimizers.rst:811
msgid "The default value is ``False``."
msgstr ""

#: ../../aqua/optimizers.rst:188
msgid ""
"A string indicating a directory for storing optimizer's parameters. If "
"``None`` then the parameters will not be stored."
msgstr ""

#: ../../aqua/optimizers.rst:195
msgid "The default value is `''`."
msgstr ""

msgid "Declarative Name"
msgstr ""

#: ../../aqua/optimizers.rst:199
msgid ""
"When referring to ADAM declaratively inside Aqua, its code ``name``, by "
"which Aqua dynamically discovers and loads it, is ``ADAM``."
msgstr ""

#: ../../aqua/optimizers.rst:207
msgid "Analytic Quantum Gradient Descent (AQGD)"
msgstr ""

#: ../../aqua/optimizers.rst:208
msgid ""
"Analytic Quantum Gradient Descent (AQGD) performs gradient descent "
"optimization with a momentum term and analytic gradients for parametrized"
" quantum gates, i.e. Pauli Rotations. See e.g.:"
msgstr ""

#: ../../aqua/optimizers.rst:212
msgid ""
"K. Mitarai, M. Negoro, M. Kitagawa, and K. Fujii. (2018). Quantum "
"circuit learning.Phys. Rev. A 98, 032309."
msgstr ""

#: ../../aqua/optimizers.rst:215
msgid ""
"Maria Schuld, Ville Bergholm, Christian Gogolin, Josh Izaac, Nathan "
"Killoran. (2019). Evaluating analytic gradients on quantum hardware. "
"Phys. Rev. A 99, 032331."
msgstr ""

#: ../../aqua/optimizers.rst:218
msgid "for further details on analytic gradients of parametrized quantum gates."
msgstr ""

#: ../../aqua/optimizers.rst:228
msgid "This parameters takes a positive ``int`` value.  The default is ``1000``."
msgstr ""

#: ../../aqua/optimizers.rst:233
msgid "eta : float"
msgstr ""

#: ../../aqua/optimizers.rst:235
msgid "The default value is ``3.0``."
msgstr ""

#: ../../aqua/optimizers.rst:245
msgid "A Boolean value indicating whether or not to display convergence messages."
msgstr ""

#: ../../aqua/optimizers.rst:254
msgid ""
"Bias towards the previous gradient momentum. Must be within the bounds: "
"[0,1) .. code:: python"
msgstr ""

#: ../../aqua/optimizers.rst:257
msgid "momentum : float"
msgstr ""

#: ../../aqua/optimizers.rst:259
msgid "The default value is ``0.25``."
msgstr ""

#: ../../aqua/optimizers.rst:263
msgid ""
"When referring to AQGD declaratively inside Aqua, its code ``name``, by "
"which Aqua dynamically discovers and loads it, is ``AQGD``."
msgstr ""

#: ../../aqua/optimizers.rst:273
msgid "Conjugate Gradient (CG) Method"
msgstr ""

#: ../../aqua/optimizers.rst:274
msgid ""
"CG is an algorithm for the numerical solution of systems of linear "
"equations whose matrices are symmetric and positive-definite. It is an "
"*iterative algorithm* in that it uses an initial guess to generate a "
"sequence of improving approximate solutions for a problem, in which each "
"approximation is derived from the previous ones.  It is often used to "
"solve unconstrained optimization problems, such as energy minimization."
msgstr ""

#: ../../aqua/optimizers.rst:282 ../../aqua/optimizers.rst:340
msgid "The maximum number of iterations to perform:"
msgstr ""

#: ../../aqua/optimizers.rst:290 ../../aqua/optimizers.rst:348
msgid "A Boolean value indicating whether or not to print convergence messages:"
msgstr ""

#: ../../aqua/optimizers.rst:298
msgid ""
"A tolerance value that must be greater than the gradient norm before "
"successful termination."
msgstr ""

#: ../../aqua/optimizers.rst:305
msgid "The default value is ``1e-05``."
msgstr ""

#: ../../aqua/optimizers.rst:308 ../../aqua/optimizers.rst:364
#: ../../aqua/optimizers.rst:492 ../../aqua/optimizers.rst:604
#: ../../aqua/optimizers.rst:659
msgid "The tolerance for termination:"
msgstr ""

#: ../../aqua/optimizers.rst:314
msgid ""
"This parameter is optional.  If specified, the value of this parameter "
"must be a ``float`` value, otherwise, it is set to ``None``.  The default"
" is ``None``."
msgstr ""

#: ../../aqua/optimizers.rst:323 ../../aqua/optimizers.rst:858
msgid "The default value is ``1.4901161193847656e-08``."
msgstr ""

#: ../../aqua/optimizers.rst:327
msgid ""
"When referring to CG declaratively inside Aqua, its code ``name``, by "
"which Aqua dynamically discovers and loads it, is ``CG``."
msgstr ""

#: ../../aqua/optimizers.rst:334
msgid "Constrained Optimization BY Linear Approximation (COBYLA)"
msgstr ""

#: ../../aqua/optimizers.rst:336
msgid ""
"COBYLA is a numerical optimization method for constrained problems where "
"the derivative of the objective function is not known. COBYLA supports "
"the following parameters:"
msgstr ""

#: ../../aqua/optimizers.rst:346 ../../aqua/optimizers.rst:404
#: ../../aqua/optimizers.rst:473
msgid "A positive ``int`` value is expected.  The default is ``1000``."
msgstr ""

#: ../../aqua/optimizers.rst:356
msgid "Reasonable initial changes to the variable:"
msgstr ""

#: ../../aqua/optimizers.rst:362
msgid "The default value is ``1.0``."
msgstr ""

#: ../../aqua/optimizers.rst:370
msgid ""
"This parameter is optional.  If specified, the value of this parameter "
"must be of type ``float``, otherwise, it is set to ``None``. The default "
"is ``None``."
msgstr ""

#: ../../aqua/optimizers.rst:375
msgid ""
"When referring to COBYLA declaratively inside Aqua, its code ``name``, by"
" which Aqua dynamically discovers and loads it, is ``COBYLA``."
msgstr ""

#: ../../aqua/optimizers.rst:382
msgid "Limited-memory Broyden-Fletcher-Goldfarb-Shanno Bound (L-BFGS-B)"
msgstr ""

#: ../../aqua/optimizers.rst:384
msgid ""
"The target goal of L-BFGS-B is to minimize the value of a differentiable "
"scalar function :math:`f`. This optimizer is a *quasi-Newton method*, "
"meaning that, in contrast to *Newtons's method*, it does not require "
":math:`f`'s *Hessian* (the matrix of :math:`f`'s second derivatives) when"
" attempting to compute :math:`f`'s minimum value. Like BFGS, L-BFGS is an"
" iterative method for solving unconstrained, non-linear optimization "
"problems, but approximates BFGS using a limited amount of computer "
"memory. L-BFGS starts with an initial estimate of the optimal value, and "
"proceeds iteratively to refine that estimate with a sequence of better "
"estimates. The derivatives of :math:`f` are used to identify the "
"direction of steepest descent, and also to form an estimate of the "
"Hessian matrix (second derivative) of :math:`f`. L-BFGS-B extends L-BFGS "
"to handle simple, per-variable bound constraints."
msgstr ""

#: ../../aqua/optimizers.rst:398
msgid "The maximum number of function evaluations:"
msgstr ""

#: ../../aqua/optimizers.rst:406 ../../aqua/optimizers.rst:458
#: ../../aqua/optimizers.rst:569 ../../aqua/optimizers.rst:634
msgid "The maximum number of iterations:"
msgstr ""

#: ../../aqua/optimizers.rst:412
msgid "A positive ``int`` value is expected.  The default is ``10``."
msgstr ""

#: ../../aqua/optimizers.rst:414
msgid ""
"An ``int`` value controlling the frequency of the printed output showing "
"the  optimizer's operations:"
msgstr ""

#: ../../aqua/optimizers.rst:421
msgid "The default is ``-1``."
msgstr ""

#: ../../aqua/optimizers.rst:423
msgid "Step size used if numerically calculating the gradient."
msgstr ""

#: ../../aqua/optimizers.rst:429 ../../aqua/optimizers.rst:674
msgid "The default value is ``1e-08``."
msgstr ""

#: ../../aqua/optimizers.rst:432
msgid ""
"Further detailed information on ``factr`` and ``iprint`` may be found at "
"`scipy.optimize.fmin_l_bfgs_b "
"<https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html>`__."
msgstr ""

#: ../../aqua/optimizers.rst:437
msgid ""
"When referring to L-BFGS-B declaratively inside Aqua, its code ``name``, "
"by which Aqua dynamically discovers and loads it, is ``L_BFGS_B``."
msgstr ""

#: ../../aqua/optimizers.rst:444
msgid "Nelder-Mead"
msgstr ""

#: ../../aqua/optimizers.rst:446
msgid ""
"The Nelder-Mead algorithm performs unnconstrained optimization; it "
"ignores bounds or constraints.  It is used to find the minimum or maximum"
" of an objective function in a multidimensional space.  It is based on "
"the Simplex algorithm. Nelder-Mead is robust in many applications, "
"especially when the first and second derivatives of the objective "
"function are not known. However, if the numerical computation of the "
"derivatives can be trusted to be accurate, other algorithms using the "
"first and/or second derivatives information might be preferred to Nelder-"
"Mead for their better performance in the general case, especially in "
"consideration of the fact that the Nelder–Mead technique is a heuristic "
"search method that can converge to non-stationary points."
msgstr ""

#: ../../aqua/optimizers.rst:464
msgid ""
"This parameter is optional.  If specified, the value of this parameter "
"must be a positive ``int``, otherwise, it is  ``None``. The default is "
"``None``."
msgstr ""

#: ../../aqua/optimizers.rst:467 ../../aqua/optimizers.rst:579
msgid "The maximum number of functional evaluations to perform:"
msgstr ""

#: ../../aqua/optimizers.rst:475 ../../aqua/optimizers.rst:587
#: ../../aqua/optimizers.rst:642
msgid "A ``bool`` value indicating whether or not to print convergence messages:"
msgstr ""

#: ../../aqua/optimizers.rst:481 ../../aqua/optimizers.rst:505
#: ../../aqua/optimizers.rst:593 ../../aqua/optimizers.rst:648
msgid "The default is ``False``."
msgstr ""

#: ../../aqua/optimizers.rst:483 ../../aqua/optimizers.rst:595
msgid ""
"A tolerance parameter indicating the absolute error in ``xopt`` between "
"iterations that will be considered acceptable for convergence."
msgstr ""

#: ../../aqua/optimizers.rst:490 ../../aqua/optimizers.rst:602
msgid "The default value is ``0.0001``."
msgstr ""

#: ../../aqua/optimizers.rst:498 ../../aqua/optimizers.rst:610
msgid ""
"This parameter is optional.  If specified, the value of this parameter "
"must be of type ``float``, otherwise, it is  ``None``. The default is "
"``None``."
msgstr ""

#: ../../aqua/optimizers.rst:507
msgid "If true will adapt algorithm to dimensionality of problem."
msgstr ""

#: ../../aqua/optimizers.rst:511
msgid ""
"When referring to Nelder-Mead declaratively inside Aqua, its code "
"``name``, by which Aqua dynamically discovers and loads it, is "
"``NELDER_MEAD``."
msgstr ""

#: ../../aqua/optimizers.rst:518
msgid "Parallel Broyden-Fletcher-Goldfarb-Shann (P-BFGS)"
msgstr ""

#: ../../aqua/optimizers.rst:520
msgid ""
"P-BFGS is a parallellized version of `L-BFGS-B <#limited-memory-broyden-"
"fletcher-goldfarb-shanno-bound-l-bfgs-b>`__, with which it shares the "
"same parameters. P-BFGS can be useful when the target hardware is a "
"quantum simulator running on a classical machine. This allows the "
"multiple processes to use simulation to potentially reach a minimum "
"faster. The parallelization may help the optimizer avoid getting stuck at"
" local optima.  In addition to the parameters of L-BFGS-B, P-BFGS "
"supports an following parameter --- the maximum number of processes "
"spawned by P-BFGS:"
msgstr ""

#: ../../aqua/optimizers.rst:534
msgid ""
"By default, P-BFGS runs one optimization in the current process and "
"spawns additional processes up to the number of processor cores. An "
"``int`` value may be specified to limit the total number of processes (or"
" cores) used.  This parameter is optional.  If specified, the value of "
"this parameter must be a positive ``int``, otherwise, it is ``None``.  "
"The default is ``None``."
msgstr ""

#: ../../aqua/optimizers.rst:542
msgid ""
"The parallel processes do not currently work for this optimizer on the "
"Microsoft Windows platform. There, P-BFGS will just run the one "
"optimization in the main process, without spawning new processes. "
"Therefore, the resulting behavior will be the same as the L-BFGS-B "
"optimizer."
msgstr ""

#: ../../aqua/optimizers.rst:550
msgid ""
"When referring to P-BFGS declaratively inside Aqua, its code ``name``, by"
" which Aqua dynamically discovers and loads it, is ``P_BFGS``."
msgstr ""

#: ../../aqua/optimizers.rst:558
msgid "Powell"
msgstr ""

#: ../../aqua/optimizers.rst:560
msgid ""
"The Powell algorithm performs unconstrained optimization; it ignores "
"bounds or constraints. Powell is a *conjugate direction method*: it "
"performs sequential one-dimensional minimization along each directional "
"vector, which is updated at each iteration of the main minimization loop."
" The function being minimized need not be differentiable, and no "
"derivatives are taken."
msgstr ""

#: ../../aqua/optimizers.rst:575
msgid ""
"This parameter is optional. If specified, the value of this parameter "
"must be a positive ``int``, otherwise, it is  ``None``. The default is "
"``None``."
msgstr ""

#: ../../aqua/optimizers.rst:585 ../../aqua/optimizers.rst:716
msgid "A positive ``int`` value is expected.  The default value is ``1000``."
msgstr ""

#: ../../aqua/optimizers.rst:615
msgid ""
"When referring to Powell declaratively inside Aqua, its code ``name``, by"
" which Aqua dynamically discovers and loads it, is ``POWELL``."
msgstr ""

#: ../../aqua/optimizers.rst:622
msgid "Sequential Least SQuares Programming (SLSQP)"
msgstr ""

#: ../../aqua/optimizers.rst:624
msgid ""
"SLSQP minimizes a function of several variables with any combination of "
"bounds, equality and inequality constraints. The method wraps the SLSQP "
"Optimization subroutine originally implemented by Dieter Kraft. SLSQP is "
"ideal for  mathematical problems for which the objective function and the"
" constraints are twice continuously differentiable. Note that the wrapper"
" handles infinite values in bounds by converting them into large floating"
" values."
msgstr ""

#: ../../aqua/optimizers.rst:640 ../../aqua/optimizers.rst:804
msgid "A positive ``int`` value is expected.  The default is ``100``."
msgstr ""

#: ../../aqua/optimizers.rst:650
msgid ""
"A tolerance value indicating precision goal for the value of the "
"objective function in the stopping criterion."
msgstr ""

#: ../../aqua/optimizers.rst:657
msgid "A ``float`` value is expected.  The default value is ``1e-06``."
msgstr ""

#: ../../aqua/optimizers.rst:665
msgid ""
"This parameter is optional.  If specified, the value of this parameter "
"must be a ``float``, otherwise, it is  ``None``. The default is ``None``."
msgstr ""

#: ../../aqua/optimizers.rst:678
msgid ""
"When referring to SLSQP declaratively inside Aqua, its code ``name``, by "
"which Aqua dynamically discovers and loads it, is ``SLSQP``."
msgstr ""

#: ../../aqua/optimizers.rst:685
msgid "Simultaneous Perturbation Stochastic Approximation (SPSA)"
msgstr ""

#: ../../aqua/optimizers.rst:687
msgid ""
"SPSA is an algorithmic method for optimizing systems with multiple "
"unknown parameters. As an optimization method, it is appropriately suited"
" to large-scale population models, adaptive modeling, and simulation "
"optimization."
msgstr ""

#: ../../aqua/optimizers.rst:692
msgid ""
"Many examples are presented at the `SPSA Web site "
"<http://www.jhuapl.edu/SPSA>`__."
msgstr ""

#: ../../aqua/optimizers.rst:694
msgid ""
"SPSA is a descent method capable of finding global minima, sharing this "
"property with other methods as simulated annealing. Its main feature is "
"the gradient approximation, which requires only two measurements of the "
"objective function, regardless of the dimension of the optimization "
"problem."
msgstr ""

#: ../../aqua/optimizers.rst:701
msgid ""
"SPSA can be used in the presence of noise, and it is therefore indicated "
"in situations involving measurement uncertainty on a quantum computation "
"when finding a minimum. If you are executing a variational algorithm "
"using a Quantum ASseMbly Language (QASM) simulator or a real device, SPSA"
" would be the most recommended choice among the optimizers provided here."
msgstr ""

#: ../../aqua/optimizers.rst:706
msgid ""
"The optimization process includes a calibration phase, which requires "
"additional functional evaluations.  Overall, the following parameters are"
" supported:"
msgstr ""

#: ../../aqua/optimizers.rst:709
msgid ""
"Maximum number of trial steps to be taken for the optimization. There are"
" two function evaluations per trial:"
msgstr ""

#: ../../aqua/optimizers.rst:718
msgid ""
"An ``int`` value determining how often optimization outcomes should be "
"stored during execution:"
msgstr ""

#: ../../aqua/optimizers.rst:724
msgid ""
"A positive ``int`` value is expected. SPSA will store optimization "
"outcomes every ``save_steps`` trial steps.  The default value is ``1``."
msgstr ""

#: ../../aqua/optimizers.rst:727
msgid ""
"The number of last updates of the variables to average on for the final "
"objective function:"
msgstr ""

#: ../../aqua/optimizers.rst:734
msgid "A positive ``int`` value is expected.  The default value is ``1``."
msgstr ""

#: ../../aqua/optimizers.rst:736
msgid "Control parameters for SPSA:"
msgstr ""

#: ../../aqua/optimizers.rst:746
msgid ""
"These are the SPSA control parameters, consisting of 5 ``float`` values, "
"and are used as described below."
msgstr ""

#: ../../aqua/optimizers.rst:749
msgid ""
"SPSA updates the parameters (``theta``) for the objective function "
"(``J``) through the following equation at iteration ``k``:"
msgstr ""

#: ../../aqua/optimizers.rst:761
msgid ""
"``J(theta)`` is the  objective value of ``theta``. ``c0``, ``c1``, "
"``c2``, ``c3`` and ``c4`` are the five control parameters. By default, "
"``c0`` is calibrated through a few evaluations on the objective function "
"with the initial ``theta``. ``c1``, ``c2``, ``c3`` and ``c4`` are set as "
"``0.1``, ``0.602``, ``0.101``, ``0.0``, respectively."
msgstr ""

#: ../../aqua/optimizers.rst:768
msgid "Calibration step for SPSA."
msgstr ""

#: ../../aqua/optimizers.rst:774
msgid ""
"The default value is ``False``. When calibration is done, i.e. when "
"``skip_calibration`` is ``False`` (by default) the control parameter "
"``c0`` as supplied is adjusted by the calibration step before "
"optimization. If ``skip_calibration`` is ``True`` then the calibration "
"step, which occurs ahead of optimization, is skipped and ``c0`` will be "
"used unaltered."
msgstr ""

#: ../../aqua/optimizers.rst:783
msgid ""
"When referring to SPSA declaratively inside Aqua, its code ``name``, by "
"which Aqua dynamically discovers and loads it, is ``SPSA``."
msgstr ""

#: ../../aqua/optimizers.rst:790
msgid "Truncated Newton (TNC)"
msgstr ""

#: ../../aqua/optimizers.rst:791
msgid ""
"TNC uses a truncated Newton algorithm to minimize a function with "
"variables subject to bounds. This algorithm uses gradient information; it"
" is also called Newton Conjugate-Gradient. It differs from the "
":ref:`Conjugate Gradient (CG) Method` method as it wraps a C "
"implementation and allows each variable to be given upper and lower "
"bounds."
msgstr ""

#: ../../aqua/optimizers.rst:799
msgid "The maximum number of iterations: .. code:: python"
msgstr ""

#: ../../aqua/optimizers.rst:802
msgid "maxiter = 1 | 2 | ..."
msgstr ""

#: ../../aqua/optimizers.rst:806
msgid ""
"A Boolean value indicating whether or not to print convergence messages: "
".. code:: python"
msgstr ""

#: ../../aqua/optimizers.rst:809
msgid "disp : bool"
msgstr ""

#: ../../aqua/optimizers.rst:813
msgid "Relative precision for finite difference calculations: .. code:: python"
msgstr ""

#: ../../aqua/optimizers.rst:816
msgid "accuracy : float"
msgstr ""

#: ../../aqua/optimizers.rst:818
msgid "The default value is ``0.0``."
msgstr ""

#: ../../aqua/optimizers.rst:820
msgid ""
"A tolerance value indicating the precision goal for the value of the "
"objective function ``f`` in the stopping criterion. .. code:: python"
msgstr ""

#: ../../aqua/optimizers.rst:824
msgid "ftol : float"
msgstr ""

#: ../../aqua/optimizers.rst:826 ../../aqua/optimizers.rst:834
#: ../../aqua/optimizers.rst:843
msgid "The default value is ``-1``."
msgstr ""

#: ../../aqua/optimizers.rst:828
msgid ""
"A tolerance value indicating precision goal for the value of ``x`` in the"
" stopping criterion, after applying ``x`` scaling factors. .. code:: "
"python"
msgstr ""

#: ../../aqua/optimizers.rst:832
msgid "xtol : float"
msgstr ""

#: ../../aqua/optimizers.rst:836
msgid ""
"A tolerance value indicating precision goal for the value of the "
"projected gradient ``g`` in the stopping criterion, after applying ``x`` "
"scaling factors. .. code:: python"
msgstr ""

#: ../../aqua/optimizers.rst:841
msgid "gtol : float"
msgstr ""

#: ../../aqua/optimizers.rst:845
msgid "The tolerance for termination: .. code::"
msgstr ""

#: ../../aqua/optimizers.rst:850
msgid ""
"This parameter is optional.  If specified, the value of this parameter "
"must be a ``float``, otherwise, it is  ``None``. The default is ``None``"
msgstr ""

#: ../../aqua/optimizers.rst:853
msgid ""
"Step size used for numerical approximation of the Jacobian. .. code:: "
"python"
msgstr ""

#: ../../aqua/optimizers.rst:856
msgid "eps : float"
msgstr ""

#: ../../aqua/optimizers.rst:862
msgid ""
"When referring to TNC declaratively inside Aqua, its code ``name``, by "
"which Aqua dynamically discovers and loads it, is ``TNC``."
msgstr ""

#: ../../aqua/optimizers.rst:869
msgid "Global Optimizers"
msgstr ""

#: ../../aqua/optimizers.rst:870
msgid ""
"Aqua supports a number of classical global optimizers, all based on the "
"open-source `NonLinear optimization (NLopt) library "
"<https://nlopt.readthedocs.io>`__. Each of these optimizers uses the "
"corresponding named optimizer from NLopt. This package has native code "
"implementations and must be installed locally for these global optimizers"
" to be accessible by Aqua. Wrapper code allowing Aqua to interface these "
"optimizers is installed in the ``nlopt`` subfolder of the ``optimizers`` "
"folder."
msgstr ""

msgid "Installation of NLopt"
msgstr ""

#: ../../aqua/optimizers.rst:880
msgid ""
"The `NLopt download and installation instructions "
"<https://nlopt.readthedocs.io/en/latest/#download-and-installation>`__ "
"describe how to install NLopt."
msgstr ""

#: ../../aqua/optimizers.rst:883
msgid ""
"If you running Aqua on Windows, then you might want to refer to the "
"specific `instructions for NLopt on Windows "
"<https://nlopt.readthedocs.io/en/latest/NLopt_on_Windows/>`__."
msgstr ""

#: ../../aqua/optimizers.rst:886
msgid ""
"If you are running Aqua on a Unix-like system, first ensure that your "
"environment is set to the Python executable for which the qiskit_aqua "
"package is installed and running. Now, having downloaded and unpacked the"
" NLopt archive file (for example, ``nlopt-2.4.2.tar.gz`` for version "
"2.4.2), enter the following commands:"
msgstr ""

#: ../../aqua/optimizers.rst:897
msgid ""
"The above makes and installs the shared libraries and Python interface in"
" `/usr/local`. To have these be used by Aqua, the following commands can "
"be entered to augment the dynamic library load path and python path "
"respectively, assuming that you choose to leave these entities where they"
" were built and installed as per above commands and that you are running "
"Python 3.6:"
msgstr ""

#: ../../aqua/optimizers.rst:907
msgid ""
"The two ``export`` commands above can be pasted into the "
"``.bash_profile`` file in the user's home directory for automatic "
"execution.  Now you can run Aqua and these optimizers should be available"
" for you to use."
msgstr ""

msgid "The ``max_evals`` Parameter"
msgstr ""

#: ../../aqua/optimizers.rst:912
msgid ""
"All the NLopt optimizers are supported by a common interface, allowing "
"the optimizers to share the same common parameters. For quantum "
"variational algorithms, it is necessary to assign a value to the "
"following parameter:"
msgstr ""

#: ../../aqua/optimizers.rst:921
msgid ""
"This parameter takes a positive ``int`` as its value, indicating the "
"maximum object function evaluation.  The default value is ``1000``."
msgstr ""

#: ../../aqua/optimizers.rst:924
msgid "Currently, Aqua supplies the following global optimizers from NLOpt:"
msgstr ""

#: ../../aqua/optimizers.rst:926
msgid ":ref:`Controller Random Search (CRS) with Local Mutation`"
msgstr ""

#: ../../aqua/optimizers.rst:927
msgid ":ref:`DIviding RECTangles algorithm - Locally based (DIRECT-L)`"
msgstr ""

#: ../../aqua/optimizers.rst:928
msgid ""
":ref:`DIviding RECTangles algorithm - Locally based - RANDomized "
"(DIRECT-L-RAND)`"
msgstr ""

#: ../../aqua/optimizers.rst:929
msgid ":ref:`Evolutionary Strategy algorithm with CaucHy distribution (ESCH)`"
msgstr ""

#: ../../aqua/optimizers.rst:930
msgid ":ref:`Improved Stochastic Ranking Evolution Strategy (ISRES)`"
msgstr ""

#: ../../aqua/optimizers.rst:936
msgid "Controller Random Search (CRS) with Local Mutation"
msgstr ""

#: ../../aqua/optimizers.rst:937
msgid ""
"`CRS with local mutation "
"<http://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#controlled-"
"random-search-crs-with-local-mutation>`__ is part of the family of the "
"CRS optimizers. The CRS optimizers start with a random population of "
"points, and randomly evolve these points by heuristic rules. In the case "
"of CRS with local mutation, the evolution is a randomized version of the "
":ref:`Nelder-Mead` local optimizer."
msgstr ""

#: ../../aqua/optimizers.rst:945
msgid ""
"When referring to CRS with local mutation declaratively inside Aqua, its "
"code ``name``, by which Aqua dynamically discovers and loads it, is "
"``CRS``."
msgstr ""

#: ../../aqua/optimizers.rst:952
msgid "DIviding RECTangles algorithm - Locally based (DIRECT-L)"
msgstr ""

#: ../../aqua/optimizers.rst:954
msgid ""
"DIviding RECTangles (DIRECT) is a deterministic-search algorithms based "
"on systematic division of the search domain into increasingly smaller "
"hyperrectangles. The `DIRECT-L "
"<http://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#direct-and-"
"direct-l>`__ version is a variant of DIRECT that makes the algorithm more"
" biased towards local search, so that it is more efficient for functions "
"with few local minima."
msgstr ""

#: ../../aqua/optimizers.rst:962
msgid ""
"When referring to DIRECT-L declaratively inside Aqua, its code ``name``, "
"by which Aqua dynamically discovers and loads it, is ``DIRECT_L``."
msgstr ""

#: ../../aqua/optimizers.rst:969
msgid "DIviding RECTangles algorithm - Locally based - RANDomized (DIRECT-L-RAND)"
msgstr ""

#: ../../aqua/optimizers.rst:971
msgid ""
"`DIRECT-L-RAND <http://nlopt.readthedocs.io/en/latest/NLopt_Algorithms"
"/#direct-and-direct-l>`__ is a variant of :ref:`DIviding RECTangles "
"algorithm - Locally based (DIRECT-L)` that uses some randomization to "
"help decide which dimension to halve next in the case of near-ties."
msgstr ""

#: ../../aqua/optimizers.rst:977
msgid ""
"When referring to DIRECT-L-RAND declaratively inside Aqua, its code "
"``name``, by which Aqua dynamically discovers and loads it, is "
"``DIRECT_L_RAND``."
msgstr ""

#: ../../aqua/optimizers.rst:984
msgid "Evolutionary Strategy algorithm with CaucHy distribution (ESCH)"
msgstr ""

#: ../../aqua/optimizers.rst:986
msgid ""
"`ESCH <http://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#esch-"
"evolutionary-algorithm>`__ is an evolutionary algorithm for global "
"optimization that supports bound constraints only. Specifically, it does "
"not support nonlinear constraints."
msgstr ""

#: ../../aqua/optimizers.rst:992
msgid ""
"When referring to ESCH declaratively inside Aqua, its code ``name``, by "
"which Aqua dynamically discovers and loads it, is ``ESCH``."
msgstr ""

#: ../../aqua/optimizers.rst:999
msgid "Improved Stochastic Ranking Evolution Strategy (ISRES)"
msgstr ""

#: ../../aqua/optimizers.rst:1001
msgid ""
"`ISRES <http://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#isres-"
"improved-stochastic-ranking-evolution-strategy>`__ is an algorithm for "
"nonlinearly-constrained global optimization. It has heuristics to escape "
"local optima, even though convergence to a global optima is not "
"guaranteed. The evolution strategy is based on a combination of a "
"mutation rule and differential variation. The fitness ranking is simply "
"via the objective function for problems without nonlinear constraints. "
"When nonlinear constraints are included, the `stochastic ranking proposed"
" by Runarsson and Yao "
"<https://notendur.hi.is/^tpr/software/sres/Tec311r.pdf>`__ is employed. "
"This method supports arbitrary nonlinear inequality and equality "
"constraints, in addition to the bound constraints."
msgstr ""

#: ../../aqua/optimizers.rst:1013
msgid ""
"When referring to ISRES declaratively inside Aqua, its code ``name``, by "
"which Aqua dynamically discovers and loads it, is ``ISRES``."
msgstr ""

